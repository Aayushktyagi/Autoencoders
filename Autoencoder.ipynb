{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayushktyagi/Generative_models/blob/master/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "h1fDTkuhKRqr",
        "colab_type": "code",
        "outputId": "467620a1-dd9c-4ba0-b31b-82a3e5ef3ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1895
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "'''\n",
        "Autoencoder implementation using tensorflow\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#load mnist Datasets\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"./MNIST\",one_hot = True)\n",
        "\n",
        "#hyper parameters:\n",
        "num_input = 784\n",
        "n_layer_1 = 256\n",
        "n_layer_2 = 128\n",
        "n_layer_3 = 64\n",
        "n_epochs = 1000\n",
        "n_learning_rate = 0.001\n",
        "batch_size = 50\n",
        "display_step = 50\n",
        "\n",
        "#input placeholder\n",
        "X = tf.placeholder(tf.float32 , [None,num_input])\n",
        "\n",
        "#weights\n",
        "weights = {\n",
        "    'encoder_h1':tf.Variable(tf.random_normal([num_input,n_layer_1])),\n",
        "    'encoder_h2':tf.Variable(tf.random_normal([n_layer_1,n_layer_2])),\n",
        "    'encoder_h3':tf.Variable(tf.random_normal([n_layer_2 , n_layer_3])),\n",
        "    'decoder_h1':tf.Variable(tf.random_normal([n_layer_3,n_layer_2])),\n",
        "    'decoder_h2':tf.Variable(tf.random_normal([n_layer_2,n_layer_1])),\n",
        "    'decoder_h3':tf.Variable(tf.random_normal([n_layer_1,num_input]))\n",
        "}\n",
        "bias = {\n",
        "    'encoder_b1':tf.Variable(tf.zeros([n_layer_1])),\n",
        "    'encoder_b2':tf.Variable(tf.zeros([n_layer_2])),\n",
        "    'encoder_b3':tf.Variable(tf.zeros([n_layer_3])),\n",
        "    'decoder_b1':tf.Variable(tf.zeros([n_layer_2])),\n",
        "    'decoder_b2':tf.Variable(tf.zeros([n_layer_1])),\n",
        "    'decoder_b3':tf.Variable(tf.zeros([num_input]))\n",
        "}\n",
        "def encoder(x , weights, bias):\n",
        "    layer_1 = tf.add(tf.matmul(x,weights['encoder_h1']),bias['encoder_b1'])\n",
        "    layer_1 = tf.nn.sigmoid(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1,weights['encoder_h2']),bias['encoder_b2'])\n",
        "    layer_2 = tf.nn.sigmoid(layer_2)\n",
        "    layer_3 = tf.add(tf.matmul(layer_2 , weights['encoder_h3']),bias['encoder_b3'])\n",
        "    layer_3 = tf.nn.sigmoid(layer_3)\n",
        "\n",
        "    return layer_3\n",
        "\n",
        "def decoder(x , weights , bias):\n",
        "    layer_1 = tf.add(tf.matmul(x , weights['decoder_h1']) , bias['decoder_b1'])\n",
        "    layer_1 = tf.nn.sigmoid(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1 , weights['decoder_h2']),bias['decoder_b2'])\n",
        "    layer_2 = tf.nn.sigmoid(layer_2)\n",
        "    layer_3 = tf.add(tf.matmul(layer_2,weights['decoder_h3']),bias['decoder_b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "\n",
        "    return layer_3\n",
        "\n",
        "encoder_op = encoder(X , weights,bias)\n",
        "decoder_op = decoder(encoder_op , weights,bias)\n",
        "\n",
        "y_pred = decoder_op\n",
        "y_true = X\n",
        "#cost function\n",
        "cost = tf.reduce_mean(tf.pow(tf.subtract(y_true,y_pred),2))\n",
        "optimizer = tf.train.AdamOptimizer(n_learning_rate).minimize(cost)\n",
        "\n",
        "#initialization\n",
        "init = tf.global_variables_initializer()\n",
        "#Save model\n",
        "saver = tf.train.Saver()\n",
        "#Staring training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i  in range(n_epochs):\n",
        "        epoch_loss = 0\n",
        "        for _ in range(int(mnist.train.num_examples / batch_size)):\n",
        "            batch_x , _ = mnist.train.next_batch(batch_size)\n",
        "            #Run optimizatio and loss\n",
        "            _ , l = sess.run([optimizer,cost],feed_dict={X:batch_x})\n",
        "            epoch_loss +=l\n",
        "        print(\"epoch :{} , loss:{}\".format(i,epoch_loss))\n",
        "        #Display batch_loss\n",
        "        # if i % display_step == 0 or i ==0:\n",
        "        #     print(\"Epoch :{} , loss :{}\".format(i,l))\n",
        "    save_path = saver.save(sess,\"Model/Autoencoder.ckpt\")\n",
        "    correct = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true,1)) # y' = y then correct\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, 'float')) #cast to float correct\n",
        "    print('Accuracy: ',accuracy.eval({X:mnist.test.images, X:mnist.test.images})) #eval using x\n",
        "\t#test data\n",
        "    index = np.random.randint(mnist.test.images.shape[0], size=8) #random 8 number from test data to be predicted\n",
        "    autoencoder = sess.run(y_pred, feed_dict={X: mnist.test.images[index]}) #predicted test data using model\n",
        "    for i in range(8):\n",
        "    \tplt.imsave(\"Model/Results\"+str(i)+\".png\",np.reshape(autoencoder[i],(28,28)), cmap=plt.get_cmap('gray')) #save result\n",
        "\n",
        "\n",
        "    #Starting testing\n",
        "    # encode and decode images from test image set and visualization of reconstruction\n",
        "    n = 4\n",
        "    canvas_orignal = np.empty((28*n,28*n))\n",
        "    canvas_recon = np.empty((28*n , 28*n))\n",
        "\n",
        "    for i in range(n):\n",
        "        #Get test dataset\n",
        "        batch_x , _ = mnist.test.next_batch(n)\n",
        "        g = sess.run(decoder_op , feed_dict = {X:batch_x})\n",
        "        #Display  orignal\n",
        "        for j in range(n):\n",
        "            canvas_orignal[i*28:(i+1)*28,j*28:(j+1)*28] = batch_x[j].reshape([28,28])\n",
        "            canvas_recon[i*28:(i+1)*28,j*28:(j+1)*28] = g[j].reshape([28,28])\n",
        "\n",
        "    print(\"Orignal Images :\")\n",
        "    plt.figure(figsize = (n,n))\n",
        "    plt.imshow(canvas_orignal,origin = \"upper\" , cmap = \"gray\")\n",
        "    plt.show()\n",
        "    plt.imsave(\"Model/Results_canvas_orignal\"+\".png\",canvas_orignal)\n",
        "    #plt.savefig(\"./Results/Results_canvas_orignal.png\")\n",
        "\n",
        "    print(\"Reconstructed Images :\")\n",
        "    plt.figure(figsize = (n,n))\n",
        "    plt.imshow(canvas_recon,origin = \"upper\" , cmap = 'gray')\n",
        "    plt.show()\n",
        "    plt.imsave(\"Model/Results_canvas_recon\"+\".png\",canvas_recon)\n",
        "    # plt.savefig(\"./Results/Results_canvas_recon.png\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Extracting ./MNIST/train-images-idx3-ubyte.gz\n",
            "Extracting ./MNIST/train-labels-idx1-ubyte.gz\n",
            "Extracting ./MNIST/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./MNIST/t10k-labels-idx1-ubyte.gz\n",
            "epoch :0 , loss:1568.5187359452248\n",
            "epoch :1 , loss:118.0854884237051\n",
            "epoch :2 , loss:115.83591171354055\n",
            "epoch :3 , loss:115.1320595741272\n",
            "epoch :4 , loss:114.40039288252592\n",
            "epoch :5 , loss:113.66907443851233\n",
            "epoch :6 , loss:112.96653579175472\n",
            "epoch :7 , loss:112.10869196802378\n",
            "epoch :8 , loss:111.35482232272625\n",
            "epoch :9 , loss:110.83195753395557\n",
            "epoch :10 , loss:110.3001536205411\n",
            "epoch :11 , loss:109.8204782679677\n",
            "epoch :12 , loss:109.45594964176416\n",
            "epoch :13 , loss:109.12315694987774\n",
            "epoch :14 , loss:108.82654456794262\n",
            "epoch :15 , loss:108.5662847161293\n",
            "epoch :16 , loss:108.34916514903307\n",
            "epoch :17 , loss:108.17769184708595\n",
            "epoch :18 , loss:108.04297047108412\n",
            "epoch :19 , loss:107.91811018437147\n",
            "epoch :20 , loss:107.82485354691744\n",
            "epoch :21 , loss:107.72812305390835\n",
            "epoch :22 , loss:107.64264500886202\n",
            "epoch :23 , loss:107.56701438874006\n",
            "epoch :24 , loss:107.49255187809467\n",
            "epoch :25 , loss:107.4265179336071\n",
            "epoch :26 , loss:107.36767107248306\n",
            "epoch :27 , loss:107.3095248118043\n",
            "epoch :28 , loss:107.24672601372004\n",
            "epoch :29 , loss:107.19197588413954\n",
            "epoch :30 , loss:107.14759926497936\n",
            "epoch :31 , loss:107.09415317326784\n",
            "epoch :32 , loss:107.04905040562153\n",
            "epoch :33 , loss:107.00682633370161\n",
            "epoch :34 , loss:106.96128517389297\n",
            "epoch :35 , loss:106.91771640628576\n",
            "epoch :36 , loss:106.87787294387817\n",
            "epoch :37 , loss:106.84340177476406\n",
            "epoch :38 , loss:106.80270184576511\n",
            "epoch :39 , loss:106.77159215509892\n",
            "epoch :40 , loss:106.73375368118286\n",
            "epoch :41 , loss:106.69561886787415\n",
            "epoch :42 , loss:106.66461595892906\n",
            "epoch :43 , loss:106.63103732466698\n",
            "epoch :44 , loss:106.59599840641022\n",
            "epoch :45 , loss:106.56273962557316\n",
            "epoch :46 , loss:106.52832911163568\n",
            "epoch :47 , loss:106.49622247368097\n",
            "epoch :48 , loss:106.46711325645447\n",
            "epoch :49 , loss:106.4316663518548\n",
            "epoch :50 , loss:106.39913177490234\n",
            "epoch :51 , loss:106.37120684236288\n",
            "epoch :52 , loss:106.33949956297874\n",
            "epoch :53 , loss:106.05122846364975\n",
            "epoch :54 , loss:105.95676230639219\n",
            "epoch :55 , loss:105.92341762781143\n",
            "epoch :56 , loss:105.88297013193369\n",
            "epoch :57 , loss:105.8531963005662\n",
            "epoch :58 , loss:105.81996313482523\n",
            "epoch :59 , loss:105.78671637922525\n",
            "epoch :60 , loss:105.75674659013748\n",
            "epoch :61 , loss:105.72427670657635\n",
            "epoch :62 , loss:105.69777072221041\n",
            "epoch :63 , loss:105.66738969832659\n",
            "epoch :64 , loss:105.64009994268417\n",
            "epoch :65 , loss:105.61377557367086\n",
            "epoch :66 , loss:105.58452680706978\n",
            "epoch :67 , loss:105.55927735567093\n",
            "epoch :68 , loss:105.53509322553873\n",
            "epoch :69 , loss:105.50988744199276\n",
            "epoch :70 , loss:105.48541700839996\n",
            "epoch :71 , loss:105.46136229485273\n",
            "epoch :72 , loss:105.43889259546995\n",
            "epoch :73 , loss:105.41783253103495\n",
            "epoch :74 , loss:105.3971767872572\n",
            "epoch :75 , loss:105.37687257677317\n",
            "epoch :76 , loss:105.35862205177546\n",
            "epoch :77 , loss:105.33708290755749\n",
            "epoch :78 , loss:105.31915929168463\n",
            "epoch :79 , loss:105.30289963632822\n",
            "epoch :80 , loss:105.2834830135107\n",
            "epoch :81 , loss:105.26635286211967\n",
            "epoch :82 , loss:105.25263976305723\n",
            "epoch :83 , loss:105.23762521147728\n",
            "epoch :84 , loss:105.22241576761007\n",
            "epoch :85 , loss:105.20808298885822\n",
            "epoch :86 , loss:105.1949640661478\n",
            "epoch :87 , loss:105.1835478246212\n",
            "epoch :88 , loss:105.17136309295893\n",
            "epoch :89 , loss:105.16237205266953\n",
            "epoch :90 , loss:105.15252006053925\n",
            "epoch :91 , loss:105.14221848547459\n",
            "epoch :92 , loss:105.13077280670404\n",
            "epoch :93 , loss:105.12343943119049\n",
            "epoch :94 , loss:105.11548633128405\n",
            "epoch :95 , loss:105.10584733635187\n",
            "epoch :96 , loss:105.09720151871443\n",
            "epoch :97 , loss:105.0895938128233\n",
            "epoch :98 , loss:105.0829079374671\n",
            "epoch :99 , loss:105.0754014775157\n",
            "epoch :100 , loss:105.06839237362146\n",
            "epoch :101 , loss:105.06217137724161\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}